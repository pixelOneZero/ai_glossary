Abstract: Referring to internal primitives of a model that have meaning within a model's context, but are not easily understandable by humans.

Actuators:

Adversarial Machine Learning (AML):

Alignment:

Attention network:

Artificial General Intelligence (AGI):

Backward function:

Backward propogation:

Biases:

Binary classification: An algorithm that returns 0 or 1 provided with given input x.

Boltzmann Machine:

Chain Model:

Clustering:

Constitutional AI (CAI): A method of training AI systems using a set of rules or principles. These rules act as a "constitution" for the AI system, allowing it to operate within a socially acceptable framework.

Control theory:

Cost function: Quantifies, for parameters, the average of the sum of loss functions.

CNN: Convolutional neural network.

Densely connected:

Derivative: The derivative of a function, also referred to as the function's slope, defines the rate of change in the function's output at a given function input.

Epoch:

Explainability:

Features: Typically the columns in a dataset, each of which contains values for a feature of the data.

Feature space:

Features: Dimensions of data

Feed-forward connectivity:

Forward function:

Forward propogation:

Global optimum:

Gradient descent: Describes the rate of learning.

Ground truth: 

Hidden layers:

Hyperparameter:

Input layer:

Invertible: Referring to a non-linear function, the output (y) is difficult or impossible to map to input (x). Plotted it appears as a deeply stepped line.

Kernel: A function that computes the dot product of the transformed versions of its arguments. A kernel transforms the data into a higher-dimensional space where it becomes linearly separable or, more generally, where the transformed data has a structure that is more receptive to learning.

LSTM: Long short-term memory.

Logistic regression: An algorithm for binary classification.

Loss function: Quantifies, for a single training example, the difference between the estimate and actual result.

m: The size of the training data set

Model:

Neuron:

NLP: Natural language processing

NN: Neural network.

Notebook: An online workspace where programming languages and libraries are pre-installed and can be run as applications. Originally created for data science, but has grown in popularity. Notebooks offer the advantage of offloading memory and dependency (languages, dependencies, etc.) demands to the web, rather than one's personal computer.

Numpy: A fundamental Python library used for scientific computing.

Offline:

Online:

Opaque: Referring to deeply nonlinear data.

Optimization:

Pandas: An open-source data analysis and manipulation library.

Perceptron: See glossary.md

Re-sampling:

Recurrent connectivity:

Regression:

Regularization:

Reinforced self-training (ReST): Refer to Gulcehre et al., 2023 â€” Google

Reinforcement Learning:

Reinforcement Learning from Human Feedback (RLHF):

RelU: Rectified linear units.

RNN: Recurrent neural network.

Sampling:

Self-organizing map (SOM): A neural network trained using unsupervised learning to create a typically two-dimensional map of the supplied inputs. SOM is particularly effective for data with a large number of dimensions.

Sigmoid: Function characterized by a flat gradient on the left and right of it's graph.

Stochastic:

Supervised Learning:

Support Vector Machine (SVM):

Tensor:

Tensor processing unit (TPU): 

Text embeddings: Numerical representations of text, play a significant role in various tasks involving the identification of similar items, like Google searches, online shopping recommendations, and personalized music suggestions.

Tuning:

Unsupervised Learning:

Vector:

Weights: 
