Abstract: Referring to internal primitives of a model that have meaning within a model's context, but are not easily understandable by humans.

Actuators:

Adversarial Machine Learning (AML):

Alignment:

Attention network:

Artificial General Intelligence (AGI):

Backward function:

Backward propogation:

Biases:

Binary classification: An algorithm that returns 0 or 1 provided with given input x.

Boltzmann Machine:

Chain of Models: In machine learning pipelines, "model chaining" or "stacking" refers to a technique where multiple models are linked together in a series, with each model in the chain making predictions that are used as inputs to the next model in the chain. This can sometimes improve performance compared to using a single model.

Class: 

Clustering: Grouping of data points

Constitutional AI (CAI): A method of training AI systems using a set of rules or principles. These rules act as a "constitution" for the AI system, allowing it to operate within a socially acceptable framework.

Control theory:

Cost function: Quantifies, for parameters, the average of the sum of loss functions. The goal of machine learning is to minimize the cost function so that actual results are as close as possible to predicted results. A cost function can be defined in absolute terms, or in polynomial terms.

CNN: Convolutional neural network.

Densely connected:

Derivative: The derivative of a function, also referred to as the function's slope, defines the rate of change in the function's output at a given function input.

Dimensionality reduction:

Eigen vector:

Empirical risk:

Epoch:

Explainability:

F1 score: A metric used to evaluate the performance of a binary classification model, which is a type of model that distinguishes between two classes (often labeled as "positive" and "negative").

Features: Typically the columns in a dataset, each of which contains values for a feature of the data.

Feature extraction: The process of defining qualities of a dataset that determine, in combination with weights, the classification of data instances. Notation: phi x, ɸ(x)
Feature space:

Features: Dimensions of data

Feed-forward connectivity:

Forward function:

Forward propogation:

Gaussian:

Global optimum:

Gradient descent: Describes the rate of learning. The derivative, or slope of a function, helps determine the direction needed to minimize the cost function.

Ground truth: 

Harmonic mean: Harmonic mean is a type of average, a measure of central tendency, which is calculated as the reciprocal of the arithmetic mean of the reciprocals of a set of numbers. It is especially useful in situations where the set of values are rates or ratios.

Hidden layers:

Hyperparameter:

Hypothesis:

Input layer:

Invertible: Referring to a non-linear function, the output (y) is difficult or impossible to map to input (x). Plotted it appears as a deeply stepped line.

Kernel: A function that computes the dot product of the transformed versions of its arguments. A kernel transforms the data into a higher-dimensional space where it becomes linearly separable or, more generally, where the transformed data has a structure that is more receptive to learning.

KNN Classifier:

LSTM: Long short-term memory.

Logistic regression: An algorithm for binary classification.

Loss function: Quantifies, for a single training example, the difference between the estimate and actual result.

m: The size of the training data set

Minimum Distance Classifier:

MLP: 

Model:

Neuron:

NLP: Natural language processing

NN: Neural network.

Notebook: An online workspace where programming languages and libraries are pre-installed and can be run as applications. Originally created for data science, but has grown in popularity. Notebooks offer the advantage of offloading memory and dependency (languages, dependencies, etc.) demands to the web, rather than one's personal computer.

Numpy: A fundamental Python library used for scientific computing.

Objective function:

Offline:

Online:

Opaque: Referring to deeply nonlinear data.

Optimization:

Pandas: An open-source data analysis and manipulation library.

Perceptron: See glossary.md

Pearson Correlation Coefficient: numerical representation of the linear correlation bewteen two variables, always having a value between -1 and 1, and

Principle component analysis (PCA):

Re-sampling:

Recall: A metric used to evaluate the performance of a classification model, particularly in binary classification problems. Recall is defined as the fraction of the total amount of relevant instances that were actually retrieved. It is also known as sensitivity, hit rate, or true positive rate (TPR).

Recurrent connectivity:

Regression:

Regularization:

Reinforced self-training (ReST): Refer to Gulcehre et al., 2023 — Google

Reinforcement Learning:

Reinforcement Learning from Human Feedback (RLHF):

RelU: Rectified linear units.

Residual:

RNN: Recurrent neural network.

Sampling:

Self-organizing map (SOM): A neural network trained using unsupervised learning to create a typically two-dimensional map of the supplied inputs. SOM is particularly effective for data with a large number of dimensions.

Sigmoid: Function characterized by a flat gradient on the left and right of it's graph.

Stochastic:

Subspace projection:

Supervised Learning: Learning based on labeled data that typically is implemented as classification or regression tasks.

Support Vector Machine (SVM):

Tensor:

Tensor processing unit (TPU): 

Text embeddings: Numerical representations of text, play a significant role in various tasks involving the identification of similar items, like Google searches, online shopping recommendations, and personalized music suggestions.

Tuning:

Unsupervised Learning: Learning based on unlabeled data that typcially is implemented as clustering, association and dimension reduction tasks.

Vector:

Weights: A numerical value that defines the important of data features
