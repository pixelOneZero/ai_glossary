Abstract: Referring to internal primitives of a model that have meaning within a model's context, but are not easily understandable by humans.

Actuators:

Adversarial Machine Learning (AML):

Alignment:

Attention network:

Artificial General Intelligence (AGI):

Backward function:

Backward propogation:

Biases:

Binary classification: An algorithm that returns 0 or 1 provided with given input x.

Control theory:

Cost function: Quantifies, for parameters, the average of the sum of loss functions.

CNN: Convolutional neural network.

Densely connected:

Derivative: The derivative of a function, also referred to as the function's slope, defines the rate of change in the function's output at a given function input.

Epoch:

Explainability:

Feature space:

Features: Dimensions of data

Forward function:

Forward propogation:

Global optimum:

Gradient descent: Describes the rate of learning.

Ground truth: 

Hidden layers:

Hyperparameter:

Input layer:

Invertible: Referring to a non-linear function, the output (y) is difficult or impossible to map to input (x). Plotted it appears as a deeply stepped line.

Kernel: A function that computes the dot product of the transformed versions of its arguments. A kernel transforms the data into a higher-dimensional space where it becomes linearly separable or, more generally, where the transformed data has a structure that is more receptive to learning.

LSTM: Long short-term memory.

Logistic regression: An algorithm for binary classification.

Loss function: Quantifies, for a single training example, the difference between the estimate and actual result.

m: The size of the training data set

Model:

Neuron:

NLP: Natural language processing

NN: Neural network.

Notebook: An online workspace where programming languages and libraries are pre-installed and can be run as applications. Originally created for data science, but has grown in popularity. Notebooks offer the advantage of offloading memory and dependency (languages, dependencies, etc.) demands to the web, rather than one's personal computer.

Numpy:

Offline:

Online:

Opaque: Referring to deeply nonlinear data.

Optimization:

Pandas:

Perceptron: See glossary.md

Regression:

Regularization:

Reinforcement Learning:

RelU: Rectified linear units.

RNN: Recurrent neural network.

Self-organizing map (SOM): A neural network trained using unsupervised learning to create a typically two-dimensional map of the supplied inputs. SOM is particularly effective for data with a large number of dimensions.

Sigmoid: Function characterized by a flat gradient on the left and right of it's graph.

Stochastic:

Supervised Learning:

Support Vector Machine (SVM):

Tensor:

Tensor processing unit (TPU): 

Tuning:

Unsupervised Learning:

Vector:

Weights: 
