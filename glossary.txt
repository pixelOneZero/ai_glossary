Alignment:

Backward function:

Backward propogation:

Biases:

Binary classification: An algorithm that returns 0 or 1 provided with given input x.

Cost function: Quantifies, for parameters, the average of the sum of loss functions.

CNN: Convolutional neural network.

Densely connected:

Derivative: The derivative of a function, also referred to as the function's slope, defines the rate of change in the function's output at a given function input.

Epoch:

Feature space:

Features: Dimensions of data

Forward function:

Forward propogation:

Global optimum:

Gradient descent: Describes the rate of learning.

Hidden layers:

Hyperparameter:

Input layer:

Kernel: A function that computes the dot product of the transformed versions of its arguments. A kernel transforms the data into a higher-dimensional space where it becomes linearly separable or, more generally, where the transformed data has a structure that is more receptive to learning.

LSTM: Long short-term memory.

Logistic regression: An algorithm for binary classification.

Loss function: Quantifies, for a single training example, the difference between the estimate and actual result.

m: The size of the training data set

Neuron:

NLP: Natural language processing

NN: Neural network.

Optimization:

Perceptron: See glossary.md

Regularization:

RelU: Rectified linear units.

RNN: Recurrent neural network.

Self-organizing map (SOM): A neural network trained using unsupervised learning to create a typically two-dimensional map of the supplied inputs. SOM is particularly effective for data with a large number of dimensions.

Sigmoid: Function characterized by a flat gradient on the left and right of it's graph.

Support Vector Machine (SVM):

Tensor:

Tuning:

Weights: 
